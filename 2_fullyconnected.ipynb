{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_fullyconnected.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kR-4eNdK6lYS","colab_type":"text"},"cell_type":"markdown","source":["Deep Learning\n","=============\n","\n","Assignment 2\n","------------\n","\n","Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n","\n","The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."]},{"metadata":{"id":"JLpLa8Jt7Vu4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HrCK6e17WzV","colab_type":"text"},"cell_type":"markdown","source":["First reload the data we generated in `1_notmnist.ipynb`."]},{"metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"cellView":"both","outputId":"639c6dc4-f317-411f-e2c3-77a38ca5ae92","executionInfo":{"status":"ok","timestamp":1533630664011,"user_tz":-480,"elapsed":1928,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["pickle_file = 'notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"cell_type":"markdown","source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"cellView":"both","outputId":"166e1aa9-32d4-4274-d538-505b608d9be3","executionInfo":{"status":"ok","timestamp":1533630665112,"user_tz":-480,"elapsed":863,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"nCLVqyQ5vPPH","colab_type":"text"},"cell_type":"markdown","source":["We're first going to train a multinomial logistic regression using simple gradient descent.\n","\n","TensorFlow works like this:\n","* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n","\n","      with graph.as_default():\n","          ...\n","\n","* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n","\n","      with tf.Session(graph=graph) as session:\n","          ...\n","\n","Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"]},{"metadata":{"id":"Nfv39qvtvOl_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":153},"cellView":"both","outputId":"5f982f29-ba92-4c65-d22a-190d1325fcac","executionInfo":{"status":"ok","timestamp":1533630666764,"user_tz":-480,"elapsed":1557,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["# With gradient descent training, even this much data is prohibitive.\n","# Subset the training data for faster turnaround.\n","train_subset = 10000\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data.\n","  # Load the training, validation and test data into constants that are\n","  # attached to the graph.\n","  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n","  tf_train_labels = tf.constant(train_labels[:train_subset])\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  # These are the parameters that we are going to be training. The weight\n","  # matrix will be initialized using random values following a (truncated)\n","  # normal distribution. The biases get initialized to zero.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  # We multiply the inputs with the weight matrix, and add biases. We compute\n","  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n","  # it's very common, and it can be optimized). We take the average of this\n","  # cross-entropy across all training examples: that's our loss.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  # We are going to find the minimum of this loss using gradient descent.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  # These are not part of training, but merely here so that we can report\n","  # accuracy figures as we train.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-58acd45c85e4>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n","\n"],"name":"stdout"}]},{"metadata":{"id":"KQcL4uqISHjP","colab_type":"text"},"cell_type":"markdown","source":["Let's run this computation and iterate:"]},{"metadata":{"id":"z2cjdenH869W","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":510},"cellView":"both","outputId":"94e082e7-72e3-4b5e-974f-89e4a607ed1d","executionInfo":{"status":"ok","timestamp":1533630671891,"user_tz":-480,"elapsed":5110,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_steps = 801\n","\n","def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])\n","\n","with tf.Session(graph=graph) as session:\n","  # This is a one-time operation which ensures the parameters get initialized as\n","  # we described in the graph: random weights for the matrix, zeros for the\n","  # biases. \n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    # Run the computations. We tell .run() that we want to run the optimizer,\n","    # and get the loss value and the training predictions returned as numpy\n","    # arrays.\n","    _, l, predictions = session.run([optimizer, loss, train_prediction])\n","    if (step % 100 == 0):\n","      print('Loss at step %d: %f' % (step, l))\n","      print('Training accuracy: %.1f%%' % accuracy(\n","        predictions, train_labels[:train_subset, :]))\n","      # Calling .eval() on valid_prediction is basically like calling run(), but\n","      # just to get that one numpy array. Note that it recomputes all its graph\n","      # dependencies.\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Loss at step 0: 20.102356\n","Training accuracy: 5.6%\n","Validation accuracy: 7.8%\n","Loss at step 100: 2.330618\n","Training accuracy: 71.0%\n","Validation accuracy: 70.1%\n","Loss at step 200: 1.867211\n","Training accuracy: 74.3%\n","Validation accuracy: 73.4%\n","Loss at step 300: 1.624360\n","Training accuracy: 75.8%\n","Validation accuracy: 74.3%\n","Loss at step 400: 1.461951\n","Training accuracy: 76.7%\n","Validation accuracy: 74.8%\n","Loss at step 500: 1.341241\n","Training accuracy: 77.3%\n","Validation accuracy: 75.1%\n","Loss at step 600: 1.246238\n","Training accuracy: 78.1%\n","Validation accuracy: 75.1%\n","Loss at step 700: 1.168845\n","Training accuracy: 78.7%\n","Validation accuracy: 75.2%\n","Loss at step 800: 1.104091\n","Training accuracy: 79.3%\n","Validation accuracy: 75.5%\n","Test accuracy: 83.0%\n"],"name":"stdout"}]},{"metadata":{"id":"x68f-hxRGm3H","colab_type":"text"},"cell_type":"markdown","source":["Let's now switch to stochastic gradient descent training instead, which is much faster.\n","\n","The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."]},{"metadata":{"id":"qhPMzWYRGrzM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["batch_size = 128\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch.\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XmVZESmtG4JH","colab_type":"text"},"cell_type":"markdown","source":["Let's run it:"]},{"metadata":{"id":"FoF91pknG_YW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":408},"cellView":"both","outputId":"b5c99c10-fe53-47df-eca6-0b277d20e2d0","executionInfo":{"status":"ok","timestamp":1533630679906,"user_tz":-480,"elapsed":6749,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 18.863680\n","Minibatch accuracy: 18.0%\n","Validation accuracy: 15.8%\n","Minibatch loss at step 500: 1.367052\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 74.8%\n","Minibatch loss at step 1000: 1.195022\n","Minibatch accuracy: 76.6%\n","Validation accuracy: 76.4%\n","Minibatch loss at step 1500: 1.054106\n","Minibatch accuracy: 80.5%\n","Validation accuracy: 77.1%\n","Minibatch loss at step 2000: 1.065435\n","Minibatch accuracy: 80.5%\n","Validation accuracy: 77.6%\n","Minibatch loss at step 2500: 1.128896\n","Minibatch accuracy: 73.4%\n","Validation accuracy: 77.7%\n","Minibatch loss at step 3000: 0.998884\n","Minibatch accuracy: 74.2%\n","Validation accuracy: 78.4%\n","Test accuracy: 86.1%\n"],"name":"stdout"}]},{"metadata":{"id":"7omWxtvLLxik","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem\n","-------\n","\n","Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n","\n","---"]},{"metadata":{"id":"6tiWoCrgHmNR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["batch_size = 128\n","hidden_size = 1024\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","  \n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch\n","  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables\n","  W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n","  B1 = tf.Variable(tf.zeros([hidden_size]))\n","  \n","  W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n","  B2 = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation\n","  logits_1 = tf.matmul(tf_train_dataset, W1) + B1\n","  relu_1 = tf.nn.relu(logits_1)\n","  \n","  logits_2 = tf.matmul(relu_1, W2) + B2\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n","  \n","  # Optimizer\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data\n","  train_prediction = tf.nn.softmax(logits_2)\n","  \n","  valid_y1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + B1)\n","  valid_prediction = tf.nn.softmax(tf.matmul(valid_y1, W2) + B2)\n","  \n","  test_y1 = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + B1)\n","  test_prediction = tf.nn.softmax(tf.matmul(test_y1, W2) + B2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hndPuChkOLF6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":408},"outputId":"445b1524-b626-47b7-e772-2f1387ea9ab3","executionInfo":{"status":"ok","timestamp":1533631158684,"user_tz":-480,"elapsed":8783,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_step = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  \n","  for step in range(num_step):\n","    # Pick an offset within the training data, which has been randomized\n","    # Note: we could use better randomization across epochs\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    \n","    # Generate a minibatch\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    \n","    # Prepare a dictionary telling the session where to feed the minibatch\n","    # The key of the dictionary is the placeholder node of the graph to be fed\n","    # and the value is the numpy array to feed to it\n","    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n","    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    \n","    if (step%500 == 0):\n","      print(\"Minibatch loss at %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy; %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at 0: 415.802826\n","Minibatch accuracy: 7.8%\n","validation accuracy: 37.1%\n","Minibatch loss at 500: 8.170660\n","Minibatch accuracy: 80.5%\n","validation accuracy: 78.9%\n","Minibatch loss at 1000: 8.747550\n","Minibatch accuracy: 84.4%\n","validation accuracy: 81.1%\n","Minibatch loss at 1500: 4.392915\n","Minibatch accuracy: 79.7%\n","validation accuracy: 80.8%\n","Minibatch loss at 2000: 7.645077\n","Minibatch accuracy: 88.3%\n","validation accuracy: 79.8%\n","Minibatch loss at 2500: 6.707069\n","Minibatch accuracy: 80.5%\n","validation accuracy: 80.7%\n","Minibatch loss at 3000: 2.593763\n","Minibatch accuracy: 82.8%\n","validation accuracy: 80.5%\n","Test accuracy; 87.0%\n"],"name":"stdout"}]}]}