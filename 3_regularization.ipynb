{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_regularization.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kR-4eNdK6lYS","colab_type":"text"},"cell_type":"markdown","source":["Deep Learning\n","=============\n","\n","Assignment 3\n","------------\n","\n","Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n","\n","The goal of this assignment is to explore regularization techniques."]},{"metadata":{"id":"JLpLa8Jt7Vu4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HrCK6e17WzV","colab_type":"text"},"cell_type":"markdown","source":["First reload the data we generated in `1_notmnist.ipynb`."]},{"metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"cellView":"both","outputId":"a5ebf455-7704-43b1-e0dc-4ffe81ae0180","executionInfo":{"status":"ok","timestamp":1533695711355,"user_tz":-480,"elapsed":1926,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["pickle_file = 'notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"cell_type":"markdown","source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"cellView":"both","outputId":"959e0259-b712-4f2b-e9df-16d3c083fae3","executionInfo":{"status":"ok","timestamp":1533695712324,"user_tz":-480,"elapsed":947,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"RajPLaL_ZW6w","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sgLbUAQ1CW-1","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 1\n","---------\n","\n","Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n","\n","---"]},{"metadata":{"id":"7FoW5wOpbxsk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":153},"outputId":"dd74e77d-a5c9-4c38-c1a3-60510f72cb55","executionInfo":{"status":"ok","timestamp":1533695714747,"user_tz":-480,"elapsed":1545,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["batch_size = 128\n","beta = 0.01\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch.\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # L2 regularizer\n","  regularizer = tf.nn.l2_loss(weights)\n","  loss = loss + beta * regularizer\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-5-e3a91b98498a>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n","\n"],"name":"stdout"}]},{"metadata":{"id":"acz-tmdlbxj5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":408},"outputId":"963e5108-6b04-441e-8a67-13d3d2c164ab","executionInfo":{"status":"ok","timestamp":1533695723195,"user_tz":-480,"elapsed":8337,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 48.601952\n","Minibatch accuracy: 7.8%\n","Validation accuracy: 9.6%\n","Minibatch loss at step 500: 0.986666\n","Minibatch accuracy: 82.0%\n","Validation accuracy: 79.5%\n","Minibatch loss at step 1000: 0.880977\n","Minibatch accuracy: 79.7%\n","Validation accuracy: 81.4%\n","Minibatch loss at step 1500: 0.622092\n","Minibatch accuracy: 85.9%\n","Validation accuracy: 80.5%\n","Minibatch loss at step 2000: 0.719007\n","Minibatch accuracy: 84.4%\n","Validation accuracy: 80.1%\n","Minibatch loss at step 2500: 0.685565\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 80.3%\n","Minibatch loss at step 3000: 0.744972\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 79.9%\n","Test accuracy: 87.9%\n"],"name":"stdout"}]},{"metadata":{"id":"LSdCjL9sbGHM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["batch_size = 128\n","hidden_size = 1024\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","  \n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch\n","  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables\n","  W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n","  B1 = tf.Variable(tf.zeros([hidden_size]))\n","  \n","  W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n","  B2 = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation\n","  logits_1 = tf.matmul(tf_train_dataset, W1) + B1\n","  relu_1 = tf.nn.relu(logits_1)\n","  \n","  logits_2 = tf.matmul(relu_1, W2) + B2\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n","  \n","  # Regulization\n","  w1_regularizer = tf.nn.l2_loss(W1)\n","  w2_regularizer = tf.nn.l2_loss(W2)\n","  \n","  loss = loss + beta * (w1_regularizer + w2_regularizer)\n","  \n","  # Optimizer\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data\n","  train_prediction = tf.nn.softmax(logits_2)\n","  \n","  valid_y1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + B1)\n","  valid_prediction = tf.nn.softmax(tf.matmul(valid_y1, W2) + B2)\n","  \n","  test_y1 = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + B1)\n","  test_prediction = tf.nn.softmax(tf.matmul(test_y1, W2) + B2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bcUbxVFEbKRl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":408},"outputId":"97219f44-4ae2-4b9f-9313-e768f8e8d77f","executionInfo":{"status":"ok","timestamp":1533695735331,"user_tz":-480,"elapsed":10554,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_step = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  \n","  for step in range(num_step):\n","    # Pick an offset within the training data, which has been randomized\n","    # Note: we could use better randomization across epochs\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    \n","    # Generate a minibatch\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    \n","    # Prepare a dictionary telling the session where to feed the minibatch\n","    # The key of the dictionary is the placeholder node of the graph to be fed\n","    # and the value is the numpy array to feed to it\n","    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n","    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    \n","    if (step%500 == 0):\n","      print(\"Minibatch loss at %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy; %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at 0: 3442.979980\n","Minibatch accuracy: 10.2%\n","validation accuracy: 28.4%\n","Minibatch loss at 500: 21.340502\n","Minibatch accuracy: 82.0%\n","validation accuracy: 83.5%\n","Minibatch loss at 1000: 0.965351\n","Minibatch accuracy: 79.7%\n","validation accuracy: 82.9%\n","Minibatch loss at 1500: 0.635982\n","Minibatch accuracy: 87.5%\n","validation accuracy: 82.5%\n","Minibatch loss at 2000: 0.645032\n","Minibatch accuracy: 85.9%\n","validation accuracy: 83.0%\n","Minibatch loss at 2500: 0.645749\n","Minibatch accuracy: 85.9%\n","validation accuracy: 82.5%\n","Minibatch loss at 3000: 0.726680\n","Minibatch accuracy: 82.8%\n","validation accuracy: 82.6%\n","Test accuracy; 90.2%\n"],"name":"stdout"}]},{"metadata":{"id":"na8xX2yHZzNF","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 2\n","---------\n","Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n","\n","---"]},{"metadata":{"id":"EpUquDvHea-6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":408},"outputId":"c210399e-f9a2-4e60-9361-0c7d4ff2fa3c","executionInfo":{"status":"ok","timestamp":1533695745926,"user_tz":-480,"elapsed":10573,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_step = 3001\n","\n","#train_dataset = train_dataset[:300, :]\n","#train_labels = train_labels[:300, :]\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  \n","  for step in range(num_step):\n","    # Pick an offset within the training data, which has been randomized\n","    # Note: we could use better randomization across epochs\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    \n","    # Generate a minibatch\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    \n","    # Prepare a dictionary telling the session where to feed the minibatch\n","    # The key of the dictionary is the placeholder node of the graph to be fed\n","    # and the value is the numpy array to feed to it\n","    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n","    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    \n","    if (step%500 == 0):\n","      print(\"Minibatch loss at %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy; %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at 0: 3484.671631\n","Minibatch accuracy: 12.5%\n","validation accuracy: 28.8%\n","Minibatch loss at 500: 21.329021\n","Minibatch accuracy: 83.6%\n","validation accuracy: 83.6%\n","Minibatch loss at 1000: 0.974193\n","Minibatch accuracy: 79.7%\n","validation accuracy: 82.9%\n","Minibatch loss at 1500: 0.635238\n","Minibatch accuracy: 87.5%\n","validation accuracy: 82.5%\n","Minibatch loss at 2000: 0.648695\n","Minibatch accuracy: 85.2%\n","validation accuracy: 82.8%\n","Minibatch loss at 2500: 0.646257\n","Minibatch accuracy: 88.3%\n","validation accuracy: 82.4%\n","Minibatch loss at 3000: 0.719782\n","Minibatch accuracy: 83.6%\n","validation accuracy: 82.6%\n","Test accuracy; 90.0%\n"],"name":"stdout"}]},{"metadata":{"id":"ww3SCBUdlkRc","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 3\n","---------\n","Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n","\n","What happens to our extreme overfitting case?\n","\n","---"]},{"metadata":{"id":"oLez1Z1nf9Iu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["batch_size = 128\n","hidden_size = 1024\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","  \n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch\n","  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables\n","  W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n","  B1 = tf.Variable(tf.zeros([hidden_size]))\n","  \n","  W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n","  B2 = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation\n","  logits_1 = tf.matmul(tf_train_dataset, W1) + B1\n","  relu_1 = tf.nn.relu(logits_1)\n","  \n","  # Drop out for relu layer\n","  keep_prob = tf.placeholder(\"float\")\n","  dropout_logits_1 = tf.nn.dropout(relu_1, keep_prob)\n","  \n","  logits_2 = tf.matmul(dropout_logits_1, W2) + B2\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n","  \n","  # Regulization\n","  w1_regularizer = tf.nn.l2_loss(W1)\n","  w2_regularizer = tf.nn.l2_loss(W2)\n","  \n","  loss = loss + beta * (w1_regularizer + w2_regularizer)\n","  \n","  # Optimizer\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data\n","  train_prediction = tf.nn.softmax(logits_2)\n","  \n","  valid_y1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + B1)\n","  valid_prediction = tf.nn.softmax(tf.matmul(valid_y1, W2) + B2)\n","  \n","  test_y1 = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + B1)\n","  test_prediction = tf.nn.softmax(tf.matmul(test_y1, W2) + B2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b6uTp8yXhf4h","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":479},"outputId":"b98066fd-acf1-4d31-bd83-41205a410b60","executionInfo":{"status":"ok","timestamp":1533695759173,"user_tz":-480,"elapsed":11716,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","    tf.initialize_all_variables().run()\n","    print(\"Initialized\")\n","    for step in range(num_steps):\n","        # Pick an offset within the training data, which has been randomized.\n","        # Note: we could use better randomization across epochs.\n","        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","        # Generate a minibatch.\n","        batch_data = train_dataset[offset:(offset + batch_size), :]\n","        batch_labels = train_labels[offset:(offset + batch_size), :]\n","        # Prepare a dictionary telling the session where to feed the minibatch.\n","        # The key of the dictionary is the placeholder node of the graph to be fed,\n","        # and the value is the numpy array to feed to it.\n","        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n","        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","        if (step % 500 == 0):\n","            print(\"Minibatch loss at step {}: {}\".format(step, l))\n","            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n","            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n","    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n","Initialized\n","Minibatch loss at step 0: 3557.66625977\n","Minibatch accuracy: 13.3\n","Validation accuracy: 30.8\n","Minibatch loss at step 500: 21.5077571869\n","Minibatch accuracy: 80.5\n","Validation accuracy: 82.9\n","Minibatch loss at step 1000: 1.01610136032\n","Minibatch accuracy: 78.9\n","Validation accuracy: 82.5\n","Minibatch loss at step 1500: 0.718991041183\n","Minibatch accuracy: 85.2\n","Validation accuracy: 82.1\n","Minibatch loss at step 2000: 0.750685751438\n","Minibatch accuracy: 82.8\n","Validation accuracy: 82.3\n","Minibatch loss at step 2500: 0.724893212318\n","Minibatch accuracy: 85.2\n","Validation accuracy: 82.1\n","Minibatch loss at step 3000: 0.830392837524\n","Minibatch accuracy: 82.0\n","Validation accuracy: 82.4\n","Test accuracy: 89.8\n"],"name":"stdout"}]},{"metadata":{"id":"-b1hTz3VWZjw","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 4\n","---------\n","\n","Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n","\n","One avenue you can explore is to add multiple layers.\n","\n","Another one is to use learning rate decay:\n","\n","    global_step = tf.Variable(0)  # count the number of steps taken.\n","    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"," \n"," ---\n"]},{"metadata":{"id":"h9SVAyj3Xi5E","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import math"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dwSnumVVjKH-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["batch_size = 128\n","beta = 0.001\n","hidden_size_1 = 1024\n","hidden_size_2 = 512\n","hidden_size_3 = 256\n","hidden_size_4 = 128\n","hidden_size_5 = 64\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","  \n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch\n","  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables\n","  W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size_1], stddev=math.sqrt(2.0/(image_size*image_size))))\n","  B1 = tf.Variable(tf.zeros([hidden_size_1]))\n","  \n","  W2 = tf.Variable(tf.truncated_normal([hidden_size_1, hidden_size_2], stddev=math.sqrt(2.0/hidden_size_1)))\n","  B2 = tf.Variable(tf.zeros([hidden_size_2]))\n","  \n","  W3 = tf.Variable(tf.truncated_normal([hidden_size_2, hidden_size_3], stddev=math.sqrt(2.0/hidden_size_2)))\n","  B3 = tf.Variable(tf.zeros([hidden_size_3]))\n","  \n","  W4 = tf.Variable(tf.truncated_normal([hidden_size_3, hidden_size_4], stddev=math.sqrt(2.0/hidden_size_3)))\n","  B4 = tf.Variable(tf.zeros([hidden_size_4]))\n","  \n","  W5 = tf.Variable(tf.truncated_normal([hidden_size_4, hidden_size_5], stddev=math.sqrt(2.0/hidden_size_4)))\n","  B5 = tf.Variable(tf.zeros([hidden_size_5]))\n","  \n","  W6 = tf.Variable(tf.truncated_normal([hidden_size_5, num_labels], stddev=math.sqrt(2.0/hidden_size_5)))\n","  B6 = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Drop out for relu layer\n","  keep_prob = tf.placeholder(\"float\")\n","  \n","  # Training computation\n","  logits_1 = tf.matmul(tf_train_dataset, W1) + B1\n","  relu_1 = tf.nn.relu(logits_1)\n","  dropout_logits_1 = tf.nn.dropout(relu_1, keep_prob)\n","  \n","  logits_2 = tf.matmul(dropout_logits_1, W2) + B2\n","  relu_2 = tf.nn.relu(logits_2)\n","  dropout_logits_2 = tf.nn.dropout(relu_2, keep_prob)\n","  \n","  logits_3 = tf.matmul(dropout_logits_2, W3) + B3\n","  relu_3 = tf.nn.relu(logits_3)\n","  dropout_logits_3 = tf.nn.dropout(relu_3, keep_prob)\n","  \n","  logits_4 = tf.matmul(dropout_logits_3, W4) + B4\n","  relu_4 = tf.nn.relu(logits_4)\n","  dropout_logits_4 = tf.nn.dropout(relu_4, keep_prob)\n","  \n","  logits_5 = tf.matmul(dropout_logits_4, W5) + B5\n","  relu_5 = tf.nn.relu(logits_5)\n","  dropout_logits_5 = tf.nn.dropout(relu_5, keep_prob)\n","  \n","  logits_6 = tf.matmul(dropout_logits_5, W6) + B6\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_6))\n","  \n","  # Regulization\n","  w1_regularizer = tf.nn.l2_loss(W1)\n","  w2_regularizer = tf.nn.l2_loss(W2)\n","  w3_regularizer = tf.nn.l2_loss(W3)\n","  w4_regularizer = tf.nn.l2_loss(W4)\n","  w5_regularizer = tf.nn.l2_loss(W5)\n","  w6_regularizer = tf.nn.l2_loss(W6)\n","  \n","  loss = loss + beta * (w1_regularizer + w2_regularizer + w3_regularizer + w4_regularizer + w5_regularizer + w6_regularizer)\n","  \n","  # Optimizer\n","  global_step = tf.Variable(0)\n","  learning_rate = tf.train.exponential_decay(0.5, global_step, 100000, 0,96)\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n","  \n","  # Predictions for the training, validation, and test data\n","  train_prediction = tf.nn.softmax(logits_6)\n","  \n","  valid_y1 = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + B1)\n","  valid_y2 = tf.nn.relu(tf.matmul(valid_y1, W2) + B2)\n","  valid_y3 = tf.nn.relu(tf.matmul(valid_y2, W3) + B3)\n","  valid_y4 = tf.nn.relu(tf.matmul(valid_y3, W4) + B4)\n","  valid_y5 = tf.nn.relu(tf.matmul(valid_y4, W5) + B5)\n","  valid_prediction = tf.nn.softmax(tf.matmul(valid_y5, W6) + B6)\n","  \n","  test_y1 = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + B1)\n","  test_y2 = tf.nn.relu(tf.matmul(test_y1, W2) + B2)\n","  test_y3 = tf.nn.relu(tf.matmul(test_y2, W3) + B3)\n","  test_y4 = tf.nn.relu(tf.matmul(test_y3, W4) + B4)\n","  test_y5 = tf.nn.relu(tf.matmul(test_y4, W5) + B5)\n","  test_prediction = tf.nn.softmax(tf.matmul(test_y5, W6) + B6)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YO83v1hTkDMY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1581},"outputId":"921bd21e-3542-4abf-c4ff-373d75bd60a3","executionInfo":{"status":"ok","timestamp":1533695851016,"user_tz":-480,"elapsed":89025,"user":{"displayName":"choi simon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"113448899521638549980"}}},"cell_type":"code","source":["num_steps = 15000\n","\n","with tf.Session(graph=graph) as session:\n","    tf.initialize_all_variables().run()\n","    print(\"Initialized\")\n","    for step in range(num_steps):\n","        # Pick an offset within the training data, which has been randomized.\n","        # Note: we could use better randomization across epochs.\n","        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","        # Generate a minibatch.\n","        batch_data = train_dataset[offset:(offset + batch_size), :]\n","        batch_labels = train_labels[offset:(offset + batch_size), :]\n","        # Prepare a dictionary telling the session where to feed the minibatch.\n","        # The key of the dictionary is the placeholder node of the graph to be fed,\n","        # and the value is the numpy array to feed to it.\n","        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n","        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","        if (step % 500 == 0):\n","            print(\"Minibatch loss at step {}: {}\".format(step, l))\n","            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n","            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n","    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 5.13097953796\n","Minibatch accuracy: 14.1\n","Validation accuracy: 10.5\n","Minibatch loss at step 500: 1.99946212769\n","Minibatch accuracy: 75.8\n","Validation accuracy: 81.3\n","Minibatch loss at step 1000: 1.39313006401\n","Minibatch accuracy: 81.2\n","Validation accuracy: 83.2\n","Minibatch loss at step 1500: 1.12482500076\n","Minibatch accuracy: 85.9\n","Validation accuracy: 83.3\n","Minibatch loss at step 2000: 0.968376278877\n","Minibatch accuracy: 87.5\n","Validation accuracy: 83.8\n","Minibatch loss at step 2500: 0.835720121861\n","Minibatch accuracy: 88.3\n","Validation accuracy: 84.6\n","Minibatch loss at step 3000: 0.917559564114\n","Minibatch accuracy: 85.2\n","Validation accuracy: 84.7\n","Minibatch loss at step 3500: 0.993162035942\n","Minibatch accuracy: 81.2\n","Validation accuracy: 82.3\n","Minibatch loss at step 4000: 0.824833750725\n","Minibatch accuracy: 85.2\n","Validation accuracy: 85.1\n","Minibatch loss at step 4500: 0.727626383305\n","Minibatch accuracy: 85.2\n","Validation accuracy: 85.0\n","Minibatch loss at step 5000: 0.898515820503\n","Minibatch accuracy: 82.8\n","Validation accuracy: 84.5\n","Minibatch loss at step 5500: 0.669452250004\n","Minibatch accuracy: 89.1\n","Validation accuracy: 84.7\n","Minibatch loss at step 6000: 0.966833770275\n","Minibatch accuracy: 82.8\n","Validation accuracy: 83.9\n","Minibatch loss at step 6500: 0.866430222988\n","Minibatch accuracy: 82.8\n","Validation accuracy: 85.9\n","Minibatch loss at step 7000: 0.743111252785\n","Minibatch accuracy: 85.2\n","Validation accuracy: 85.8\n","Minibatch loss at step 7500: 0.851215720177\n","Minibatch accuracy: 82.0\n","Validation accuracy: 85.7\n","Minibatch loss at step 8000: 0.931869983673\n","Minibatch accuracy: 77.3\n","Validation accuracy: 85.2\n","Minibatch loss at step 8500: 0.783217906952\n","Minibatch accuracy: 85.2\n","Validation accuracy: 85.1\n","Minibatch loss at step 9000: 0.759075880051\n","Minibatch accuracy: 87.5\n","Validation accuracy: 86.0\n","Minibatch loss at step 9500: 0.731281161308\n","Minibatch accuracy: 84.4\n","Validation accuracy: 85.8\n","Minibatch loss at step 10000: 0.876614391804\n","Minibatch accuracy: 85.2\n","Validation accuracy: 85.9\n","Minibatch loss at step 10500: 0.655362010002\n","Minibatch accuracy: 88.3\n","Validation accuracy: 85.4\n","Minibatch loss at step 11000: 0.759981095791\n","Minibatch accuracy: 83.6\n","Validation accuracy: 85.5\n","Minibatch loss at step 11500: 0.725065648556\n","Minibatch accuracy: 87.5\n","Validation accuracy: 85.3\n","Minibatch loss at step 12000: 0.890948414803\n","Minibatch accuracy: 82.0\n","Validation accuracy: 86.0\n","Minibatch loss at step 12500: 0.77883040905\n","Minibatch accuracy: 84.4\n","Validation accuracy: 86.0\n","Minibatch loss at step 13000: 0.776694893837\n","Minibatch accuracy: 85.2\n","Validation accuracy: 86.0\n","Minibatch loss at step 13500: 0.781928539276\n","Minibatch accuracy: 84.4\n","Validation accuracy: 85.8\n","Minibatch loss at step 14000: 0.88285189867\n","Minibatch accuracy: 82.0\n","Validation accuracy: 85.2\n","Minibatch loss at step 14500: 0.890141189098\n","Minibatch accuracy: 79.7\n","Validation accuracy: 85.7\n","Test accuracy: 93.1\n"],"name":"stdout"}]}]}